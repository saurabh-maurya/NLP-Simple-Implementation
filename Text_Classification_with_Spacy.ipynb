{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification with Spacy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPVxIW+dh/QAQOGmtO04AyM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzTV7lGhn6RI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Loading the spam data\n",
        "# ham is the label for non-spam messages\n",
        "\n",
        "spam = pd.read_csv('../input/nlp-course/spam.csv')\n",
        "spam.head(10)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV88OjRyoJ5J",
        "colab_type": "text"
      },
      "source": [
        "Machine learning models don't learn from raw text data. Instead, you need to convert the text to something numeric.\n",
        "\n",
        "The simplest common representation is a variation of one-hot encoding.\n",
        "\n",
        "spaCy handles the **bag of words** conversion and building a simple linear model for you with the **TextCategorizer** class\n",
        "\n",
        "```\n",
        "# Bag of Words\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Create an empty model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Create the TextCategorizer with exclusive classes and \"bow\" architecture\n",
        "textcat = nlp.create_pipe(\n",
        "              \"textcat\",\n",
        "              config={\n",
        "                \"exclusive_classes\": True,\n",
        "                \"architecture\": \"bow\"})\n",
        "\n",
        "# Add the TextCategorizer to the empty model\n",
        "nlp.add_pipe(textcat)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8YqURWNoJ2V",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Add labels to text classifier\n",
        "textcat.add_label(\"ham\")\n",
        "textcat.add_label(\"spam\")\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6d2o7LIoJ0E",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# training a text categorizer model\n",
        "\n",
        "train_texts = spam['text'].values\n",
        "train_labels = [{'cats': {'ham': label == 'ham',\n",
        "                          'spam': label == 'spam'}} \n",
        "                for label in spam['label']]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVoC0oBEoJwa",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# combine the texts and labels into a single list.\n",
        "\n",
        "train_data = list(zip(train_texts, train_labels))\n",
        "train_data[:3]\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGgQYnz4oJpx",
        "colab_type": "text"
      },
      "source": [
        "ready to train the model\n",
        " \n",
        "create an optimizer using **nlp.begin_training()** ... spaCy uses this optimizer to update the model\n",
        "\n",
        "In general it's more efficient to train models in small batches ... spaCy provides the **minibatch** function that returns a generator yielding minibatches for training\n",
        "\n",
        "the minibatches are split into texts and labels, then used with **nlp.update** to update the model's parameters\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from spacy.util import minibatch\n",
        "\n",
        "spacy.util.fix_random_seed(1)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "# Create the batch generator with batch size = 8\n",
        "batches = minibatch(train_data, size=8)\n",
        "# Iterate through minibatches\n",
        "for batch in batches:\n",
        "    # Each batch is a list of (text, label) but we need to\n",
        "    # send separate lists for texts and labels to update().\n",
        "    # This is a quick way to split a list of tuples into lists\n",
        "    texts, labels = zip(*batch)\n",
        "    nlp.update(texts, labels, sgd=optimizer)\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7IOZNFAoJd4",
        "colab_type": "text"
      },
      "source": [
        "This is just one training loop (or epoch) through the data\n",
        "The model will typically need multiple epochs\n",
        "Use another loop for more epochs, and optionally re-shuffle the training data at the begining of each loop\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import random\n",
        "\n",
        "random.seed(1)\n",
        "spacy.util.fix_random_seed(1)\n",
        "optimizer = nlp.begin_training()\n",
        "\n",
        "losses = {}\n",
        "for epoch in range(10):\n",
        "    random.shuffle(train_data)\n",
        "    # Create the batch generator with batch size = 8\n",
        "    batches = minibatch(train_data, size=8)\n",
        "    # Iterate through minibatches\n",
        "    for batch in batches:\n",
        "        # Each batch is a list of (text, label) but we need to\n",
        "        # send separate lists for texts and labels to update().\n",
        "        # This is a quick way to split a list of tuples into lists\n",
        "        texts, labels = zip(*batch)\n",
        "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
        "    print(losses)\n",
        "    \n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHwAC9U8uLx_",
        "colab_type": "text"
      },
      "source": [
        "**Making Predictions**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
        "         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n",
        "docs = [nlp.tokenizer(text) for text in texts]\n",
        "    \n",
        "# Use textcat to get the scores for each doc\n",
        "textcat = nlp.get_pipe('textcat')\n",
        "scores, _ = textcat.predict(docs)\n",
        "\n",
        "print(scores)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4_X8LsV12G7",
        "colab_type": "text"
      },
      "source": [
        "The scores are used to predict a single class or label by choosing the label with the highest probability\n",
        "You get the index of the highest probability with **scores.argmax**, then use the index to get the label string from **textcat.labels**\n",
        "\n",
        "```\n",
        "predicted_labels = scores.argmax(axis=1)\n",
        "print([textcat.labels[label] for label in predicted_labels])\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkbseKlP11iv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}