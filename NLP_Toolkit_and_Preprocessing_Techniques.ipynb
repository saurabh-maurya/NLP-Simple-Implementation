{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Toolkit and Preprocessing Techniques.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVGdgax491CPM/IA2ZlXUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurabh-maurya/NLP-Simple-Implementation/blob/master/NLP_Toolkit_and_Preprocessing_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hel-e8bKikB_",
        "colab_type": "text"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_Mp_-eGhQEn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5999d866-d86d-44ea-8dc4-c23ecd8cb283"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Package abc is already up-to-date!\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Package alpino is already up-to-date!\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Package biocreative_ppi is already up-to-date!\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Package brown is already up-to-date!\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Package brown_tei is already up-to-date!\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Package cess_cat is already up-to-date!\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Package cess_esp is already up-to-date!\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Package chat80 is already up-to-date!\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Package city_database is already up-to-date!\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Package cmudict is already up-to-date!\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package comparative_sentences is already up-to-date!\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       |   Package comtrans is already up-to-date!\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Package conll2000 is already up-to-date!\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Package conll2002 is already up-to-date!\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       |   Package conll2007 is already up-to-date!\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Package crubadan is already up-to-date!\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Package dependency_treebank is already up-to-date!\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Package dolch is already up-to-date!\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Package europarl_raw is already up-to-date!\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Package floresta is already up-to-date!\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Package framenet_v15 is already up-to-date!\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Package framenet_v17 is already up-to-date!\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Package gazetteers is already up-to-date!\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Package genesis is already up-to-date!\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Package gutenberg is already up-to-date!\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Package ieer is already up-to-date!\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Package inaugural is already up-to-date!\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Package indian is already up-to-date!\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       |   Package jeita is already up-to-date!\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Package kimmo is already up-to-date!\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       |   Package knbc is already up-to-date!\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Package lin_thesaurus is already up-to-date!\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Package mac_morpho is already up-to-date!\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       |   Package machado is already up-to-date!\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       |   Package masc_tagged is already up-to-date!\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Package moses_sample is already up-to-date!\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Package movie_reviews is already up-to-date!\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Package names is already up-to-date!\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       |   Package nombank.1.0 is already up-to-date!\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Package nps_chat is already up-to-date!\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Package omw is already up-to-date!\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Package opinion_lexicon is already up-to-date!\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Package paradigms is already up-to-date!\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Package pil is already up-to-date!\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Package pl196x is already up-to-date!\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Package ppattach is already up-to-date!\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Package problem_reports is already up-to-date!\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       |   Package propbank is already up-to-date!\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Package ptb is already up-to-date!\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Package product_reviews_1 is already up-to-date!\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Package product_reviews_2 is already up-to-date!\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Package pros_cons is already up-to-date!\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Package qc is already up-to-date!\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       |   Package reuters is already up-to-date!\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Package rte is already up-to-date!\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       |   Package semcor is already up-to-date!\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Package senseval is already up-to-date!\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Package sentiwordnet is already up-to-date!\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Package sentence_polarity is already up-to-date!\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Package shakespeare is already up-to-date!\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Package sinica_treebank is already up-to-date!\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Package smultron is already up-to-date!\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Package state_union is already up-to-date!\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Package stopwords is already up-to-date!\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Package subjectivity is already up-to-date!\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Package swadesh is already up-to-date!\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Package switchboard is already up-to-date!\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Package timit is already up-to-date!\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Package toolbox is already up-to-date!\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Package treebank is already up-to-date!\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Package twitter_samples is already up-to-date!\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Package udhr is already up-to-date!\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Package udhr2 is already up-to-date!\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Package unicode_samples is already up-to-date!\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package universal_treebanks_v20 is already up-to-date!\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Package verbnet is already up-to-date!\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Package verbnet3 is already up-to-date!\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Package webtext is already up-to-date!\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Package wordnet is already up-to-date!\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Package wordnet_ic is already up-to-date!\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Package words is already up-to-date!\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Package ycoe is already up-to-date!\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Package rslp is already up-to-date!\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package maxent_treebank_pos_tagger is already up-to-date!\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Package universal_tagset is already up-to-date!\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Package maxent_ne_chunker is already up-to-date!\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Package punkt is already up-to-date!\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Package book_grammars is already up-to-date!\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Package sample_grammars is already up-to-date!\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Package spanish_grammars is already up-to-date!\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Package basque_grammars is already up-to-date!\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Package large_grammars is already up-to-date!\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Package tagsets is already up-to-date!\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       |   Package snowball_data is already up-to-date!\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Package word2vec_sample is already up-to-date!\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       |   Package panlex_swadesh is already up-to-date!\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Package mte_teip5 is already up-to-date!\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package averaged_perceptron_tagger is already up-to-date!\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package averaged_perceptron_tagger_ru is already up-to-\n",
            "       |       date!\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Package perluniprops is already up-to-date!\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Package nonbreaking_prefixes is already up-to-date!\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       |   Package vader_lexicon is already up-to-date!\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Package porter_test is already up-to-date!\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Package wmt15_eval is already up-to-date!\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Package mwa_ppdb is already up-to-date!\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1asAdncKjxvq",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHBYx5O2j2MQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Tokenization\n",
        " - Turn text into a meaningful format for analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLTVfT6xiz19",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "32c208e3-6135-4cd0-e649-c55e819ccf5d"
      },
      "source": [
        "# Tokenization (Words)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\" \n",
        "print(word_tokenize(my_text))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi', 'Mr.', 'Smith', '!', 'I', '’', 'm', 'going', 'to', 'buy', 'some', 'vegetables', '(', 'tomatoes', 'and', 'cucumbers', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIrNKf_Qlgya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62172642-5cdf-4891-d5bc-e883261320c5"
      },
      "source": [
        "# Tokenization (Sentences)\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\" \n",
        "print(sent_tokenize(my_text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi Mr. Smith!', 'I’m going to buy some vegetables (tomatoes and cucumbers) from the store.', 'Should I pick up some black-eyed peas as well?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbydYasAloto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9868589b-1967-410a-ab57-0475fa91c593"
      },
      "source": [
        "# Tokenization (N-Grams)\n",
        "\n",
        "from nltk.util import ngrams\n",
        "my_words = word_tokenize(my_text) # This is the list of all words\n",
        "twograms = list(ngrams(my_words,2)) # This is for two-word combos, but can pick any n\n",
        "print(twograms)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Hi', 'Mr.'), ('Mr.', 'Smith'), ('Smith', '!'), ('!', 'I'), ('I', '’'), ('’', 'm'), ('m', 'going'), ('going', 'to'), ('to', 'buy'), ('buy', 'some'), ('some', 'vegetables'), ('vegetables', '('), ('(', 'tomatoes'), ('tomatoes', 'and'), ('and', 'cucumbers'), ('cucumbers', ')'), (')', 'from'), ('from', 'the'), ('the', 'store'), ('store', '.'), ('.', 'Should'), ('Should', 'I'), ('I', 'pick'), ('pick', 'up'), ('up', 'some'), ('some', 'black-eyed'), ('black-eyed', 'peas'), ('peas', 'as'), ('as', 'well'), ('well', '?')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HAozk3dloel",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8ceb73cf-d0f0-4968-9b9b-525bc9b02bc0"
      },
      "source": [
        "# Tokenization (Regular Expressions)\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\" \n",
        "whitespace_tokenize = RegexpTokenizer(\"\\s+\", gaps = True)\n",
        "print(whitespace_tokenize.tokenize(my_text))\n",
        "\n",
        "# RegexpTokenizer to match only capitalized words\n",
        "cap_tokenizer = RegexpTokenizer(\"[A-Z]['\\w]+\")\n",
        "print(cap_tokenizer.tokenize(my_text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi', 'Mr.', 'Smith!', 'I’m', 'going', 'to', 'buy', 'some', 'vegetables', '(tomatoes', 'and', 'cucumbers)', 'from', 'the', 'store.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well?']\n",
            "['Hi', 'Mr', 'Smith', 'Should']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv1RDIc5kRpe",
        "colab_type": "text"
      },
      "source": [
        "# 2. Clean the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucwWeo6knekr",
        "colab_type": "text"
      },
      "source": [
        "**1. Remove: capital letters, punctuation, numbers, stop words**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZtsRCO9kUUZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "abbad4aa-261c-462b-c693-9ac2a21cae36"
      },
      "source": [
        "# Replace Punctuations With A White Space\n",
        "\n",
        "import re # Regular expression library\n",
        "import string\n",
        "clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
        "print(clean_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi Mr  Smith  I’m going to buy some vegetables  tomatoes and cucumbers  from the store  Should I pick up some black eyed peas as well \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL9SP0BLkf1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37d39443-3844-46f6-cd1a-5ae2488ce3cb"
      },
      "source": [
        "# Make All Text Lowercase\n",
        "\n",
        "clean_text = clean_text.lower()\n",
        "clean_text\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'hi mr  smith  i’m going to buy some vegetables  tomatoes and cucumbers  from the store  should i pick up some black eyed peas as well '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSoQk_ahoLMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7cdccc91-6a85-4e60-e9f1-580d14cc22ba"
      },
      "source": [
        "# Removes all words containing digits\n",
        "\n",
        "clean_text = re.sub('\\w*\\d\\w*', ' ', clean_text)\n",
        "clean_text"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'hi mr  smith  i’m going to buy some vegetables  tomatoes and cucumbers  from the store  should i pick up some black eyed peas as well '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YCyhP_jo5WE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "0212300c-5f12-4529-d970-2edaf24c8eb7"
      },
      "source": [
        "#  Stop Words\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\"]\n",
        "\n",
        "# Incorporate stop words when creating the count vectorizer\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "X = cv.fit_transform(my_text)\n",
        "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>black</th>\n",
              "      <th>buy</th>\n",
              "      <th>cucumbers</th>\n",
              "      <th>eyed</th>\n",
              "      <th>going</th>\n",
              "      <th>hi</th>\n",
              "      <th>mr</th>\n",
              "      <th>peas</th>\n",
              "      <th>pick</th>\n",
              "      <th>smith</th>\n",
              "      <th>store</th>\n",
              "      <th>tomatoes</th>\n",
              "      <th>vegetables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   black  buy  cucumbers  eyed  going  ...  pick  smith  store  tomatoes  vegetables\n",
              "0      1    1          1     1      1  ...     1      1      1         1           1\n",
              "\n",
              "[1 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4JCzJw_qQWK",
        "colab_type": "text"
      },
      "source": [
        "**2. Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFzM3NhbpZ35",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "f7f09f5d-3d77-40fd-b7a2-89dd56a06331"
      },
      "source": [
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "# Try some stems\n",
        "print('drive: {}'.format(stemmer.stem('drive')))\n",
        "print('drives: {}'.format(stemmer.stem('drives')))\n",
        "print('driver: {}'.format(stemmer.stem('driver')))\n",
        "print('drivers: {}'.format(stemmer.stem('drivers')))\n",
        "print('driven: {}'.format(stemmer.stem('driven')))\n",
        "\n",
        "words = cv.get_feature_names()\n",
        "for word in words:\n",
        "  print(stemmer.stem(word), end = \" \")\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive: driv\n",
            "drives: driv\n",
            "driver: driv\n",
            "drivers: driv\n",
            "driven: driv\n",
            "black buy cucumb ey going hi mr pea pick smi stor tomato veget "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6XX7S4mtRk5",
        "colab_type": "text"
      },
      "source": [
        "**3. Parts of Speech Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kYMVDkCqzeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f4f5234c-d361-40ef-f41b-52553e835407"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables (tomatoes and cucumbers) from the store. Should I pick up some black-eyed peas as well?\" \n",
        "tokens = pos_tag(word_tokenize(my_text))\n",
        "print(tokens)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Hi', 'NNP'), ('Mr.', 'NNP'), ('Smith', 'NNP'), ('!', '.'), ('I', 'PRP'), ('’', 'VBP'), ('m', 'RB'), ('going', 'VBG'), ('to', 'TO'), ('buy', 'VB'), ('some', 'DT'), ('vegetables', 'NNS'), ('(', '('), ('tomatoes', 'NNS'), ('and', 'CC'), ('cucumbers', 'NNS'), (')', ')'), ('from', 'IN'), ('the', 'DT'), ('store', 'NN'), ('.', '.'), ('Should', 'MD'), ('I', 'PRP'), ('pick', 'VB'), ('up', 'RP'), ('some', 'DT'), ('black-eyed', 'JJ'), ('peas', 'NNS'), ('as', 'IN'), ('well', 'RB'), ('?', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovA3rHEFtiOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6afd8d1c-8ad5-4cca-ca1c-0102c9c5e499"
      },
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUCOEZnnt4Gf",
        "colab_type": "text"
      },
      "source": [
        "**4.  Named Entity Recognition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ygj3VGKtrVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from nltk.chunk import ne_chunk\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
        "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
        "entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_mcZhVowZRY",
        "colab_type": "text"
      },
      "source": [
        "**5. Compound Term Extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmZJuafkuEto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4be1c930-5bd5-4a95-e73e-9d16da4c953c"
      },
      "source": [
        "from nltk.tokenize import MWETokenizer # multi-word expression\n",
        "my_text = \"You all are the greatest students of all time.\"\n",
        "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
        "mwe_tokens"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WefpPQeCwgBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}